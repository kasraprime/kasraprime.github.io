<!doctype html>

<html lang="en" dir="ltr"></html>
  <head>
  <meta charset="utf-8">
<title>Nlp - Blog of Kasra Darvish</title>
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="generator" content="Hugo 0.101.0" /><meta property="og:site_name" content="Blog of Kasra Darvish">
  <meta property="og:title" content="Nlp">
  <meta property="og:description" content="A blog about books, life, AI, and Ph.D. life">
  <meta property="description" content="A blog about books, life, AI, and Ph.D. life">
  <meta property="og:url" content="https://kasraprime.com/blog/blogposts/nlp/">
  <meta property="og:type" content="article">
  
    <meta property="og:image" content="https://kasraprime.com/blog/img/kasra.JPG">
  
  <link rel="stylesheet" href="/blog/css/bundle.min.9da1009965aef1af97d00c4f0dc221726fe188fa0e4c4e1757453e1478f7ff3c.css" integrity="sha256-naEAmWWu8a&#43;X0AxPDcIhcm/hiPoOTE4XV0U&#43;FHj3/zw="><link rel="stylesheet" href="/blog/css/add-on.css">
</head>

  <body>
    

<header id="site-header">
  <nav id="site-nav">
    <h1 class="nav-title">
      <a href="/" class="nav">
        
          Nlp
        
      </a>
    </h1>
    <menu id="site-nav-menu" class="flyout-menu menu">
      
        
          
          <a href="https://www.kasraprime.com" class="nav link"><i class='fa fa-home'></i> Home</a>
        
      
        
          
          <a href="/blog/about" class="nav link"><i class='far fa-id-card'></i> About</a>
        
      
        
          
          <a href="https://www.kasraprime.com/CV_Kasra_Darvish.pdf" class="nav link"><i class='far fa-id-card'></i> CV</a>
        
      
        
          
          <a href="/blog/blogposts" class="nav link"><i class='far fa-newspaper'></i> Blog</a>
        
      
        
          
          <a href="/blog/categories" class="nav link"><i class='fas fa-sitemap'></i> Categories</a>
        
      
        
          
          <a href="/blog/contact" class="nav link"><i class='far fa-envelope'></i> Contact</a>
        
      
      <a href="#share-menu" class="nav link share-toggle"><i class="fas fa-share-alt">&nbsp;</i>Share</a>
      <a href="#search-input" class="nav link search-toggle"><i class="fas fa-search">&nbsp;</i>Search</a>
    </menu>
    <a href="#search-input" class="nav search-toggle"><i class="fas fa-search fa-2x">&nbsp;</i></a>
    <a href="#share-menu" class="nav share-toggle"><i class="fas fa-share-alt fa-2x">&nbsp;</i></a>
    <a href="#lang-menu" class="nav lang-toggle" lang="en">en</a>
    <a href="#site-nav" class="nav nav-toggle"><i class="fas fa-bars fa-2x"></i></a>
  </nav>
  <menu id="search" class="menu"><input id="search-input" class="search-input menu"></input><div id="search-results" class="search-results menu"></div></menu>
  <menu id="lang-menu" class="flyout-menu menu">
  <a href="#" lang="en" class="nav link active">English (en)</a>
  
    
      
    
      
        <a href="/blog/fa" lang="fa" class="nav no-lang link">Farsi (fa)</a>
      
    
  
</menu>

  
    <menu id="share-menu" class="flyout-menu menu">
      <h1>Share Post</h1>
      




  
    
    <a href="//twitter.com/share?text=check out the blogpost Nlp written by @KasraPrime at&amp;url=https%3a%2f%2fkasraprime.com%2fblog%2fblogposts%2fnlp%2f" target="_blank" rel="noopener" class="nav share-btn twitter">
        <p>Twitter</p>
      </a>
  

  
      <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fkasraprime.com%2fblog%2fblogposts%2fnlp%2f" target="_blank" rel="noopener" class="nav share-btn facebook">
        <p>Facebook</p>
        </a>
  

  
    <a href="//www.reddit.com/submit?url=https%3a%2f%2fkasraprime.com%2fblog%2fblogposts%2fnlp%2f&amp;title=Nlp" target="_blank" rel="noopener" class="nav share-btn reddit">
          <p>Reddit</p>
        </a>
  

  
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fkasraprime.com%2fblog%2fblogposts%2fnlp%2f&amp;title=Nlp" target="_blank" rel="noopener" class="nav share-btn linkedin">
            <p>LinkedIn</p>
          </a>
  

  
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fkasraprime.com%2fblog%2fblogposts%2fnlp%2f&amp;description=Nlp" target="_blank" rel="noopener" class="nav share-btn pinterest">
          <p>Pinterest</p>
        </a>
  

  
        <a href="mailto:?subject=Check%20out%20this%20post%20by %5bKasra%20Darvish%5d&amp;body=https%3a%2f%2fkasraprime.com%2fblog%2fblogposts%2fnlp%2f" target="_blank" class="nav share-btn email" data-proofer-ignore>
          <p>Email</p>
        </a>
  


    </menu>
  
</header>

    <div id="wrapper">
      <section id="site-intro" >
  <a href="/"><img src="https://kasraprime.com/blog/img/kasra.JPG" class="circle" width="250" alt="This is me" /></a>
  <header>
    <h1>Kasra Darvish</h1>
  </header>
  <main>
    <p>I write to exist beyond time!</p>
  </main>
  
    <footer>
      <ul class="socnet-icons">
        

        <li><a href="//github.com/kasraprime" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//www.linkedin.com/in/kasraprime" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>




<li><a href="//facebook.com/kasraprime" target="_blank" rel="noopener" title="Facebook" class="fab fa-facebook"></a></li>








<li><a href="//instagram.com/kasraprime" target="_blank" rel="noopener" title="Instagram" class="fab fa-instagram"></a></li>

<li><a href="//twitter.com/kasraprime" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>






<li><a href="//scholar.google.com/citations?user=8jcoBdwAAAAJ" target="_blank" rel="noopener" title="Google Scholar"><i class="ai ai-google-scholar"></i></a></li>






      </ul>
    </footer>
  
</section>

      <main id="site-main">
        
  <article>
    <div class="post">
      <header>
  <div class="title">
    
      <h2><a href="/blog/blogposts/nlp/">Nlp</a></h2>
    
    
  </div>
  <div class="meta">
    <time datetime="2023-08-08 00:00:00 &#43;0000 UTC">August 8, 2023</time>
    <p>[Kasra Darvish]</p>
    <p>4-Minute Read</p>
  </div>
</header>

          <div class="content">
        
        <p>Q: What is a joint probability?</p>
<p>We have formal languages like programming languages, and natural languages like English. Natural languages are not formally specified, therefore we need statistical models to learn from examples, and approximate the natural lang.</p>
<p>the number of times a term occurs in a document is called its <strong>term frequency</strong></p>
<p>Language modeling is the task of <strong>assigning a probability</strong> to sentences in a language. The notion of a language model is inherently probabilistic. A language model is a function that puts a probability measure over strings drawn from some vocabulary. A language model can be developed and used standalone, such as to generate new sequences of text that appear to have come from the corpus.</p>
<p>we have 2 categories of language models: 1- statistical (probabilistic) models 2- Neural language models</p>
<p>Typically, neural net language models are constructed and trained as <a href="https://en.wikipedia.org/wiki/Probabilistic_classifier">probabilistic classifiers</a> that learn to predict a probability distribution</p>
<p>Based on <a href="https://medium.com/analytics-vidhya/introduction-to-natural-language-processing-part-1-777f972cc7b3">https://medium.com/analytics-vidhya/introduction-to-natural-language-processing-part-1-777f972cc7b3</a>:</p>
<p>The process of turning <strong>text into numbers</strong> is commonly known as <strong>vectorization</strong> or <strong>embedding techniques</strong></p>
<p>We can measure how <strong>similar</strong> two words by measuring the <strong>angles between the vectors</strong> or by examining their dot product.</p>
<p>We can also map documents, characters or groups of words to vectors as well.</p>
<p><strong>Document</strong> is a term that gets thrown around a lot in the NLP field. It refers to an <strong>unbroken entity of text</strong>, usually one that is of interest to the analysis task. For example, if you are trying to create an algorithm to identify spam emails, each email would be its own document.</p>
<p>Vectorizing groups of words helps us differentiate between words with more than one semantic meaning (capturing the context). For example, &ldquo;crash&rdquo; can refer to a &ldquo;car crash&rdquo; or a &ldquo;stock market crash&rdquo; or intruding into a party.</p>
<p>The underlying mechanism to creating these vectors is by examining the <strong>context</strong> in which these words appear. We can examine <strong>how often a certain word appears</strong> in each document, or how often two words <strong>co-occur</strong> together.</p>
<p>All of these embedding techniques are reliant on the <strong>distributional hypothesis</strong>, the assumption that &ldquo;words which are used and occur in the same contexts tend to purport similar meaning&rdquo;.</p>
<p>Bag of Words (BOW): the simplest method for vectorization (embedding technique)
Cons: Bag of words is not a good representation of language, especially <strong>when you have a small vocabulary</strong>. It <strong>ignores word order</strong>, <strong>word relationships</strong> and <strong>produces sparse vectors</strong> that is largely filled with zeros.</p>
<p>in BOW, words like a, the are the most frequent, to normalize this we use tf-idf</p>
<h2 id="n-gram-model">n-gram model</h2>
<p>An <strong>n-gram model</strong> is a type of probabilistic <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> for predicting the next item in such a sequence in the form of a (<em>n</em> âˆ’ 1)&ndash;order <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov model</a>. with larger <em>n</em>, a model can store more context .</p>
<p>This idea can be traced to an experiment by <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a>&rsquo;s work in <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a>. Shannon posed the question: given a sequence of letters (for example, the sequence &ldquo;for ex&rdquo;), what is the <a href="https://en.wikipedia.org/wiki/Likelihood">likelihood</a> of the next letter? From training data, one can derive a <a href="https://en.wikipedia.org/wiki/Probability_distribution">probability distribution</a> for the next letter given a history of size n<a href="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b">https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b</a>: <em>a</em> = 0.4, <em>b</em> = 0.00001, <em>c</em> = 0, &hellip;.; where the probabilities of all possible &ldquo;next-letters&rdquo; <strong>sum to 1.0</strong>.</p>
<p>More concisely, an <em>n</em>-gram model predicts \(x_i\) based on \(x_{i-(n-1)},&hellip;,x_{i-1}\)</p>
<p>Pros: Two benefits of <em>n</em>-gram models (and algorithms that use them) are <strong>simplicity</strong> and <strong>scalability</strong></p>
<h2 id="bag-of-words">bag of words</h2>
<p>special case of n-grams with n=1. good for word frequency.</p>
<p>we get all words in all corpus. imagine there is m words in all the documents. then we create an m-dimensional vector for each document. so if we have d documents we have d vectors each m-dimensional, or we can have an m*d matrix. in each document we count the number of times each of the m words are repeated. that&rsquo;s bow.</p>
<ul>
<li>Cons:
<ol>
<li>it is very sparse because not all the words are used in all documents.</li>
<li>having similar vectors for words that are not similar. (refer to medium post example)</li>
</ol>
</li>
</ul>
<h2 id="word2vec">Word2vec</h2>
<p><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a></p>
<p>if we have v different words in all the documents, then we have a v-dimensional one hot vector to represent each word as initialization and input to the shallow neural network. then we have a hidden layer with 300 neurons, which after training would be our word embedding. and the output of this hidden layer goes to softmax and produces the output layer which is again v-dimensional, but <strong>not one-hot vector</strong>, because it gives the probability for each word that can appear after the word given as input.so we have a v-dimensional vector with bunch of float elements representing the probability, and the sum of all v elements is 1.</p>
<h2 id="doc2vec">Doc2vec</h2>
<p><a href="https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e">https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e</a></p>

      </div>
      <footer>
        <div class="stats">
  
    <ul class="categories">
      <li>None</li>
    </ul>
  
  
    <ul class="tags">
      <li>None</li>
    </ul>
  
</div>

      </footer>
      <a href="https://www.buymeacoffee.com/kasraprime"><img src="https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&emoji=&slug=kasraprime&button_colour=FFDD00&font_colour=000000&font_family=Cookie&outline_colour=000000&coffee_colour=ffffff"></a>
      <div id="socnet-share">
        




  
    
    <a href="//twitter.com/share?text=check out the blogpost Nlp written by @KasraPrime at&amp;url=https%3a%2f%2fkasraprime.com%2fblog%2fblogposts%2fnlp%2f" target="_blank" rel="noopener" class="nav share-btn twitter">
        <p>Twitter</p>
      </a>
  

  
      <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fkasraprime.com%2fblog%2fblogposts%2fnlp%2f" target="_blank" rel="noopener" class="nav share-btn facebook">
        <p>Facebook</p>
        </a>
  

  
    <a href="//www.reddit.com/submit?url=https%3a%2f%2fkasraprime.com%2fblog%2fblogposts%2fnlp%2f&amp;title=Nlp" target="_blank" rel="noopener" class="nav share-btn reddit">
          <p>Reddit</p>
        </a>
  

  
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fkasraprime.com%2fblog%2fblogposts%2fnlp%2f&amp;title=Nlp" target="_blank" rel="noopener" class="nav share-btn linkedin">
            <p>LinkedIn</p>
          </a>
  

  
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fkasraprime.com%2fblog%2fblogposts%2fnlp%2f&amp;description=Nlp" target="_blank" rel="noopener" class="nav share-btn pinterest">
          <p>Pinterest</p>
        </a>
  

  
        <a href="mailto:?subject=Check%20out%20this%20post%20by %5bKasra%20Darvish%5d&amp;body=https%3a%2f%2fkasraprime.com%2fblog%2fblogposts%2fnlp%2f" target="_blank" class="nav share-btn email" data-proofer-ignore>
          <p>Email</p>
        </a>
  


      </div>
    </div>
    
      
  <div class='post'>
    <div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "kasraprime" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </div>



    
  </article>
  <div class="pagination">
    
      <a href="/blog/blogposts/machine-learning/" class="button left"><span>Machine Learning</span></a>
    
    
      <a href="/blog/blogposts/teaching/" class="button right"><span>Teaching</span></a>
    
  </div>

      </main>
      <section id="site-sidebar">
  
    <section id="recent-posts">
      <header>
        <h1>Recent Posts</h1>
      </header>
      
      <article class="mini-post">
          
        <header>
          <h2><a href="/blog/blogposts/large-language-models/">Large Language Models</a></h2>
          <time class="published" datetime="2024-02-08 00:00:00 &#43;0000 UTC">February 8, 2024</time>
        </header>
      </article>
      
      <article class="mini-post">
          
        <header>
          <h2><a href="/blog/blogposts/paper-localizing-active-objects-from-egocentric-vision/">Paper Localizing Active Objects From Egocentric Vision</a></h2>
          <time class="published" datetime="2024-02-08 00:00:00 &#43;0000 UTC">February 8, 2024</time>
        </header>
      </article>
      
      <article class="mini-post">
          
        <header>
          <h2><a href="/blog/blogposts/semi-live-journal/">Semi Live Journal</a></h2>
          <time class="published" datetime="2024-02-01 00:00:00 &#43;0000 UTC">February 1, 2024</time>
        </header>
      </article>
      
      <article class="mini-post">
          
        <header>
          <h2><a href="/blog/blogposts/research-workflow/">Research Workflow</a></h2>
          <time class="published" datetime="2023-08-08 16:12:23 -0400 EDT">August 8, 2023</time>
        </header>
      </article>
      
      <article class="mini-post">
          
        <header>
          <h2><a href="/blog/blogposts/machine-learning-experiment-tracking-with-wandb/">Machine Learning Experiment Tracking With Wandb</a></h2>
          <time class="published" datetime="2023-08-08 16:08:48 -0400 EDT">August 8, 2023</time>
        </header>
      </article>
      
      
        <footer>
          <a href="/blog/blogposts" class="button">See More</a>
        </footer>
      
    </section>
  

  
    

      <section id="categories">
        <header>
          <h1><a href="/blog/categories">Categories</a></h1>
        </header>
        <ul>
          
          
          <li>
              <a href="/blog/categories/research/">research<span class="count">5</span></a>
          
          <li>
              <a href="/blog/categories/presentations/">presentations<span class="count">3</span></a>
          
          <li>
              <a href="/blog/categories/machine-learning/">machine-learning<span class="count">2</span></a>
          
          <li>
              <a href="/blog/categories/science/">science<span class="count">2</span></a>
          
          <li>
              <a href="/blog/categories/updates/">updates<span class="count">2</span></a>
          
          <li>
              <a href="/blog/categories/journal/">journal<span class="count">1</span></a>
          
          <li>
              <a href="/blog/categories/writing/">writing<span class="count">1</span></a>
          
          </li>
        </ul>
      </section>
    
  

  
    <section id="mini-bio">
      <header>
        <h1>About</h1>
      </header>
      <p>I'm a Ph.D. student interested in Artificial Intelligence, Machine Learning and intelligence in its abstract form</p>
      <footer>
        <a href="/blog/about" class="button">Learn More</a>
      </footer>
    </section>
  
</section>

      <footer id="site-footer">
  
      <ul class="socnet-icons">
        

        <li><a href="//github.com/kasraprime" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//www.linkedin.com/in/kasraprime" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>




<li><a href="//facebook.com/kasraprime" target="_blank" rel="noopener" title="Facebook" class="fab fa-facebook"></a></li>








<li><a href="//instagram.com/kasraprime" target="_blank" rel="noopener" title="Instagram" class="fab fa-instagram"></a></li>

<li><a href="//twitter.com/kasraprime" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>






<li><a href="//scholar.google.com/citations?user=8jcoBdwAAAAJ" target="_blank" rel="noopener" title="Google Scholar"><i class="ai ai-google-scholar"></i></a></li>






      </ul>
  
  <p class="copyright">
    Â© 2024 Blog of Kasra Darvish
      <br>
    Theme: <a href='https://github.com/pacollins/hugo-future-imperfect-slim' target='_blank' rel='noopener'>Hugo Future Imperfect Slim</a><br>A <a href='https://html5up.net/future-imperfect' target='_blank' rel='noopener'>HTML5 UP port</a> | Powered by <a href='https://gohugo.io/' title='0.101.0' target='_blank' rel='noopener'>Hugo</a>
  </p>
</footer>
<a id="back-to-top" href="#" class="fas fa-arrow-up fa-2x"></a>

      <script src="/blog/js/highlight.js"></script>
    
    <script>hljs.highlightAll();</script><script src="/blog/js/bundle.min.a8892f2bf895a38c6790dda277e8e19ca80705a306a1f152c8d7ca95b5307488.js" integrity="sha256-qIkvK/iVo4xnkN2id&#43;jhnKgHBaMGofFSyNfKlbUwdIg="></script>
    <script src="/blog/js/add-on.js"></script>
    </div>
    <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="kasraprime" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FFDD00" data-position="left" data-x_margin="18" data-y_margin="18"></script>
  </body>
</html>
