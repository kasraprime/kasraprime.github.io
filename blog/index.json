[{
    "title": "Research Workflow",
    "date": "",
    "description": "",
    "body": "In order to be good at something, you need to have good tools. You need to hone your crafts. Research is no excemption either. The following is a talk I gave at IRAL Lab at UMBC\n",
    "ref": "/blog/blogposts/research-workflow/"
  },{
    "title": "Machine Learning Experiment Tracking With Wandb",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/blog/blogposts/machine-learning-experiment-tracking-with-wandb/"
  },{
    "title": "Research Skills",
    "date": "",
    "description": "",
    "body": "Two of most important reserach skills that are usually not thought are networking and using AI tools. I presented the follwoing talk for the IRAL lab at UMBC.\n",
    "ref": "/blog/blogposts/research-skills/"
  },{
    "title": "Art",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/blog/blogposts/art/"
  },{
    "title": "Artificial Intelligence",
    "date": "",
    "description": "",
    "body": "A branch of computer science which tries to create Intelligence in machines.\n",
    "ref": "/blog/blogposts/artificial-intelligence/"
  },{
    "title": "ChatGpt",
    "date": "",
    "description": "",
    "body": "An Artificial Intelligence chatbot and Nlp model by openAI.\n",
    "ref": "/blog/blogposts/chatgpt/"
  },{
    "title": "Classification",
    "date": "",
    "description": "",
    "body": "Why Classification As Gendler states (2011), \u0026ldquo;classifying objects into groups allows us to proceed effectively in an environment teeming with overwhelming detail.\u0026rdquo;\nRandom Baseline In order to compute the accuracy of a random baseline for a multi-class classfication you can simply: \\[acc = P(y=0) \\times P(\\hat{y}=0) + P(class is 1) \\times P(you guess 1) \\] where P(y=1) means the probability that the class is actually 1, and \\(P(\\hat{y}=1)\\) means tne probability that you guess 1.\nMajority-Class Classifier This a baseline which is equal or better than random baseline (False. It can be worst) in which we always predict the majority class. It\u0026rsquo;s also called zero rule (ZeroR or 0R).\n",
    "ref": "/blog/blogposts/classification/"
  },{
    "title": "Generative Models",
    "date": "",
    "description": "",
    "body": "Generative AI models are a broad area in Machine Learning that instead of Classification focus on creating data. Classification is about prediction which is really cool; Nostradamus ;) However, the ability to create is superior; We become Gods, new kind of gods that really exist! Also, if you can create, you can predict.\nWith that introduction, we can jump into the edge of research in this field. This field is growing really fast and has gained a lot of attention from the public and media recently, especially the famous ChatGpt model by OpenAI.\nIn this blogpost, I talk about the most recent and interesting generative models and I will update this post with the new advancements in the field. Let\u0026rsquo;s get started:\nText-to-Text Models ChatGPT Perplexity.ai perplexity.ai is an attempt to make a tool such as chatgpt more like a search engine.\nTex-to-Image Models DALL.E Imagen Text-to-Audio Models AudioGPT AudioGPT is a text-to-audio model where you provide a text prompt, and the model generates an auido regarding your promp. You can play with this model on the huggingface hub.\nMusicLM https://blog.google/technology/ai/musiclm-google-ai-test-kitchen/?utm_source=alphasignalai.beehiiv.com\u0026amp;utm_medium=newsletter\u0026amp;utm_campaign=is-this-the-end-of-regulation-free-ai\nText-to-Video Models Runway: https://research.runwayml.com/gen2?utm_source=alphasignalai.beehiiv.com\u0026amp;utm_medium=newsletter\u0026amp;utm_campaign=meta-s-new-and-powerful-text-to-music-model Stability AI stable animation https://stability.ai/blog/stable-animation-sdk?mc_cid=c59d71288a\u0026amp;mc_eid=03e4a944e9\nMultimodal Generative Models ",
    "ref": "/blog/blogposts/generative-models/"
  },{
    "title": "God",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/blog/blogposts/god/"
  },{
    "title": "Intelligence",
    "date": "",
    "description": "",
    "body": "It\u0026rsquo;s hard to define intelligence. I will do it later!\n",
    "ref": "/blog/blogposts/intelligence/"
  },{
    "title": "Machine Learning",
    "date": "",
    "description": "",
    "body": "Teaching Whenever you want to explain machine learning to someone, use the example of Teaching. Machine learning is like taking an exam from students. You compare students\u0026rsquo; results (model\u0026rsquo;s prediction) with the answers (labels) you have.\nResources https://course19.fast.ai/videos/?lesson=1 https://wandb.ai/aarora/weekly%5C_digest/reports/This-Week-in-Research-Papers--Vmlldzo2MTA1MDg?galleryTag=digests Generative vs Discriminative models https://stats.stackexchange.com/questions/12421/generative-vs-discriminative\nGenerative models provide a model of how the data is actually generated (think of them as giving you a model of both \\(P(X|Y)\\) and \\(P(Y)\\)), and discriminative algorithms as simply providing classification splits.\nNote that generative algorithms have discriminative properties, since you can get \\(P(Y|X)\\) once you have \\(P(X|Y)\\) and \\(P(Y)\\) (by Bayes\u0026rsquo; Theorem), though discriminative algorithms don\u0026rsquo;t really have generative properties.\nDiscriminative algorithms allow you to classify points, without providing a model of how the points are actually generated. So these could be either: - probabilistic algorithms try to learn \\(P(Y|X)\\) (e.g., logistic regression); - or non-probabilistic algorithms that try to learn the mappings directly from the points to the classes (e.g., perceptron and SVMs simply give you a separating hyperplane, but no model of generating new points).\nAnother way of thinking about this is that generative algorithms make some kind of structure assumptions on your model, but discriminative algorithms make fewer assumptions. For example, Naive Bayes assumes conditional independence of your features, while logistic regression (the discriminative \u0026ldquo;counterpart\u0026rdquo; of Naive Bayes) does not.\nhttps://medium.com/@SmartLabAI/a-brief-overview-of-imitation-learning-8a8a75c44a9c\nReinforcement Learning: Useful when we cannot specify a loss function, and we don\u0026rsquo;t have labeled data. Also when agent needs to take some action, it\u0026rsquo;s better to use RL. In fact in RL we even don\u0026rsquo;t have the training data, and we have to learn from experience. Imagine playing chess: there is no training data to learn from, there is no label. All we have is reward signals.\nImitation learning: we use expert demonstration/policy to train our agent to find the best policy. The twist is that if everything is perfect we can use the expert policy out of the box (rule based system), however imitation learning is useful in scenarios where we cannot demonstrate every possible state action pair such as chess, or in scenarios where data is noisy and demonstration is not going to be the same as execution like in folding clothes. Some of the methods to do imitation learning include:\nbehavioral cloning: using expert policy (P*) provided by an expert and then the loss function would be L(a*,P(s)) where a* is the action performed by the expert at state S, and P(s) is the action agent would perform given state S. We try to minimize this loss. This is supervised learning. inverse reinforcement learning is to use expert policy to find the reward function, and then find the optimal policy direct policy learning which needs interactive expert RNN Useful when dealing with sequential data like language or sound. Feed forward networks are not good fit for sequential data because they don\u0026#39;t satisfy the following properties. *RNNs are Turing complete* handle variable length sequence long term dependencies to be tracked maintain info about order share parameters across the sequence Therefore we use RNN for sequence modeling. If we unfold an RNN we could easily see how it works. we use both input and hidden to compute the output. For the first input we don\u0026rsquo;t have any hidden so either we use random or zero. Therefore we have 2 different weights to learn. one that connects input to the neuron and the other one which connects hidden to the neuron. Note that the output=hidden.\n\\(h_t=sigmoid(W_x.X+W_h.h_{t-1})\\) where \\(h_{t-1}\\) is the hidden (output) at one time step before. I just ignored bias in this formula. so \\(h_t=output_t\\)\nthe network is same for each batch starting from t=0 to t=n, then for the next batch we start from t=0, which means we start from the beginning of the network with weights already learned, and for hidden we can initialize it again. In other words every time we call model(x) in our code it starts from t=0 to t=n, and then until the next time we call model(x) that we start from the beginning of the network again\nLSTM watch this video for explanation. https://www.youtube.com/watch?v=WCUNPb-5EYI\nExploding gradients: when derivative of activation function is too large for many of them, the multiplication of these numbers would be very large\nVanishing gradients: The exact opposite situation of exploding gradients where the derivative of activation function is too small, and the multiplication becomes almost zero and therefore we cannot update the weights because we don\u0026rsquo;t have any direction to go.\nGenerative models: are type of unsupervised learning. VAE, and GAN are generative models\nAutoencoders: An encoder-decoder that use a deep network to learn a mapping from input to latent space (which is feature representations), this is the encoding phase, and using the latent features to reconstruct the same input data using the same deep network (it should be symmetric), and obviously the loss would be the difference between the reconstructed (generated) output and the original input. The second phase is the decoding part. We can think of autoencoders as dimensionality reduction methods, as well as unsupervised feature extraction methods.\nAfter training the autoencoder. we can throw away the decoder part, and use encoder to extract features, and use that to initialize a supervised model. Refer to myQuestions for why we cannot use decoder part for generation.\nIn order to be able to use the decoder of our autoencoder for generative purpose, we have to be sure that the latent space is regular enough. One possible solution to obtain such regularity is to introduce explicit regularization during the training process. A variational autoencoder can be defined as being an autoencoder whose training is regularized to avoid overfitting and ensure that the latent space has good properties that enable generative process.\nVariational Autoencoders: the probabilistic version of autoencoders that can be used to generate data. Variational Autoencoders are regularized version of autoencoders making the generative process possible, while we cannot use autoencoders to generate data. The training is the same as autoencoder with one difference which is: instead of encoding an input as a single vector, we encode it as a distribution over the latent space. The decoder then samples a data point (vector) from that distribution, and decode the point and then the rest is the same to compute reconstruction error. The loss function also has a regularization term which is to make distributions returned by the encoder close to a standard normal distribution (gaussian). This term is expressed as the KL divergence between the 2 distribution.\ngraph:\nsimple autoencoder: (deterministic) input \u0026ndash;\u0026gt; latent representation \u0026ndash;\u0026gt; reconstructed input x \u0026ndash;\u0026gt; z=e(x) \u0026ndash;\u0026gt; d(z)= d(e(x))\nvariational autoencoder: (probabilistic) input \u0026ndash;\u0026gt; latent distribution \u0026ndash;\u0026gt; sampled latent distribution \u0026ndash;\u0026gt; reconstructed input x \u0026ndash;\u0026gt; p(z|x) \u0026ndash;\u0026gt; z~p(z|x) \u0026ndash;\u0026gt; d(z)\nrefer to this post: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\nDimensionality Reduction: the process of reducing the number of features that describe some data. useful for data storage, heavy computation, and data visualization (we cannot visualize data of more than 3 dimensions). Autoencoders are also a dimensionality reduction method.\nAutoencoders w/o deep network would be an encoder/decoder which is a general paradigm for dimensionality reduction. Therefore PCA can be described as an encoder-decoder paradigm in which we try to minimize the reconstruction error which is the difference between x and d(e(x))\nPrincipal Component Analysis (PCA): A dimensionality reduction method which can also be defined in terms of encoder-decoder.\nPCA finds basis vectors called principal components. \\([1,2,3]=c_1[1,0,0]+c_2[0,1,0]+c_3[0,0,1]\\)\nEach vector here is one of the principal components. We have 3 principal components (think of them as coordinates, directions)\nNeural Turing Machine: It\u0026rsquo;s an RNN where a neural network coupled to external memory resources, and memory interaction are differentiable end-to-end.\nGaussian Mixture Models (GMM) Parametric models: A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples)\nNon-Parametric methods: In which we don\u0026rsquo;t assume any fixed number of parameters. When we don\u0026rsquo;t have any prior knowledge about data. Models include Decision Tree, kNN, SVM, ANN (feed forward).\nmulti-label classification Multi-label classification is the problem of finding a model that maps inputs x to binary vectors y (assigning a value of 0 or 1 for each element (label) in y).\nProblem transformation methods try to transform the multilabel classification into binary or multiclass classification problems. Algorithm adaptation methods adapt multiclass algorithms so they can be applied directly to the problem\nReconstructing images in generative models Problem: I wanted to show the reconstructed image from decoder, but the decoder\u0026rsquo;s output was an embedding vector, and I was struggling to visualize it.\nSolution: You cannot visualize an embedding as an RGB image. If you want to visualize your reconstructed image, you have to define your network in a way that it\u0026rsquo;s last layer of decoder has the same shape as image input (channels, width, height)\n",
    "ref": "/blog/blogposts/machine-learning/"
  },{
    "title": "Nlp",
    "date": "",
    "description": "",
    "body": "Q: What is a joint probability?\nWe have formal languages like programming languages, and natural languages like English. Natural languages are not formally specified, therefore we need statistical models to learn from examples, and approximate the natural lang.\nthe number of times a term occurs in a document is called its term frequency\nLanguage modeling is the task of assigning a probability to sentences in a language. The notion of a language model is inherently probabilistic. A language model is a function that puts a probability measure over strings drawn from some vocabulary. A language model can be developed and used standalone, such as to generate new sequences of text that appear to have come from the corpus.\nwe have 2 categories of language models: 1- statistical (probabilistic) models 2- Neural language models\nTypically, neural net language models are constructed and trained as probabilistic classifiers that learn to predict a probability distribution\nBased on https://medium.com/analytics-vidhya/introduction-to-natural-language-processing-part-1-777f972cc7b3:\nThe process of turning text into numbers is commonly known as vectorization or embedding techniques\nWe can measure how similar two words by measuring the angles between the vectors or by examining their dot product.\nWe can also map documents, characters or groups of words to vectors as well.\nDocument is a term that gets thrown around a lot in the NLP field. It refers to an unbroken entity of text, usually one that is of interest to the analysis task. For example, if you are trying to create an algorithm to identify spam emails, each email would be its own document.\nVectorizing groups of words helps us differentiate between words with more than one semantic meaning (capturing the context). For example, \u0026ldquo;crash\u0026rdquo; can refer to a \u0026ldquo;car crash\u0026rdquo; or a \u0026ldquo;stock market crash\u0026rdquo; or intruding into a party.\nThe underlying mechanism to creating these vectors is by examining the context in which these words appear. We can examine how often a certain word appears in each document, or how often two words co-occur together.\nAll of these embedding techniques are reliant on the distributional hypothesis, the assumption that \u0026ldquo;words which are used and occur in the same contexts tend to purport similar meaning\u0026rdquo;.\nBag of Words (BOW): the simplest method for vectorization (embedding technique) Cons: Bag of words is not a good representation of language, especially when you have a small vocabulary. It ignores word order, word relationships and produces sparse vectors that is largely filled with zeros.\nin BOW, words like a, the are the most frequent, to normalize this we use tf-idf\nn-gram model An n-gram model is a type of probabilistic language model for predicting the next item in such a sequence in the form of a (n − 1)\u0026ndash;order Markov model. with larger n, a model can store more context .\nThis idea can be traced to an experiment by Claude Shannon\u0026rsquo;s work in information theory. Shannon posed the question: given a sequence of letters (for example, the sequence \u0026ldquo;for ex\u0026rdquo;), what is the likelihood of the next letter? From training data, one can derive a probability distribution for the next letter given a history of size nhttps://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b: a = 0.4, b = 0.00001, c = 0, \u0026hellip;.; where the probabilities of all possible \u0026ldquo;next-letters\u0026rdquo; sum to 1.0.\nMore concisely, an n-gram model predicts \\(x_i\\) based on \\(x_{i-(n-1)},\u0026hellip;,x_{i-1}\\)\nPros: Two benefits of n-gram models (and algorithms that use them) are simplicity and scalability\nbag of words special case of n-grams with n=1. good for word frequency.\nwe get all words in all corpus. imagine there is m words in all the documents. then we create an m-dimensional vector for each document. so if we have d documents we have d vectors each m-dimensional, or we can have an m*d matrix. in each document we count the number of times each of the m words are repeated. that\u0026rsquo;s bow.\nCons: it is very sparse because not all the words are used in all documents. having similar vectors for words that are not similar. (refer to medium post example) Word2vec http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\nif we have v different words in all the documents, then we have a v-dimensional one hot vector to represent each word as initialization and input to the shallow neural network. then we have a hidden layer with 300 neurons, which after training would be our word embedding. and the output of this hidden layer goes to softmax and produces the output layer which is again v-dimensional, but not one-hot vector, because it gives the probability for each word that can appear after the word given as input.so we have a v-dimensional vector with bunch of float elements representing the probability, and the sum of all v elements is 1.\nDoc2vec https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "ref": "/blog/blogposts/nlp/"
  },{
    "title": "Teaching",
    "date": "",
    "description": "",
    "body": "Teaching is an Art to me.\n",
    "ref": "/blog/blogposts/teaching/"
  },{
    "title": "Text-to-Video AI Models: A New Hype in Multimodal Machine Learning",
    "date": "",
    "description": "",
    "body": "There has been great hype in multimodal research these past few days because of the release of multiple text-to-video models while text-to-image models are still pretty hype and not perfect yet. Diffusion models such as GLIDE in general and stable diffusion in particular are the state-of-the-art models for text-to-image tasks as of now. However, multiple papers came out recently that tackle the task of generating videos from text prompts or still images. Here are some of these models released all about the same time:\nMake-a-video by Meta Imagen video by Google Phenaki Apparantly by Google Brain (It\u0026rsquo;s under submission at ICLR anonymously, but the paper is on Arxiv) The 2-minute video example generated by Phenaki shown below is mind-blowing.\nBelow is the prompts they used to generate this video.\nLots of traffic in futuristic city. An alien spaceship arrives to the futuristic city. The camera gets inside the alien spaceship. The camera moves forward until showing an astronaut in the blue room. The astronaut is typing in the keyboard. The camera moves away from the astronaut. The astronaut leaves the keyboard and walks to the left. The astronaut leaves the keyboard and walks away. The camera moves beyond the astronaut and looks at the screen. The screen behind the astronaut displays fish swimming in the sea. Crash zoom into the blue fish. We follow the blue fish as it swims in the dark ocean. The camera points up to the sky through the water. The ocean and the coastline of a futuristic city. Crash zoom towards a futuristic skyscraper. The camera zooms into one of the many windows. We are in an office room with empty desks. A lion runs on top of the office desks. The camera zooms into the lion\u0026rsquo;s face, inside the office. Zoom out to the lion wearing a dark suit in an office room. The lion wearing looks at the camera and smiles. The camera zooms out slowly to the skyscraper exterior. Timelapse of sunset in the modern city. reference: https://phenaki.video\nThe speed of progress in the field of AI and machine learning is crazy fast. Multimodal research in particular has received a lot of attention. It was only in May 2022 that Google released Imagen which is a text-to-image model. Now in October 2022, they released the similar model for generating videos.\n",
    "ref": "/blog/blogposts/text-to-video-ai-models/"
  },{
    "title": "Regex",
    "date": "",
    "description": "",
    "body": "Basics Use this website interactively to learn. Use this website to generate regular expressions\nExclude something Use [^something] to exclude some patterns.\nInclude something Similar to excluding but [something].\nInclude one or another [one|another]\nRepeats * 0 or more of something + 1 or more of something Some Characters . Everything except new line. in or der to search for [](.org) you need to write \\[\\[.*\\]\\]. Let\u0026rsquo;s say you want to keep the content between the brackets. Then you need to use capture groups and instead you add a pair of parantheses which acts as a group and then you can reference to it. \\[\\[(.*)\\]\\] Then you can reference to it by $1. $0 is the whole pattern you searched for. So if you replace your search with $0 it basically doesn\u0026rsquo;t change anything.\nExample You want to convert [anything](anything.org) to [anything](anything.md). you can simply search for \\[\\[([^\\]]*)\\]\\] and then replace it by [$1]($1.md) since you only have 1 capture group.\nWe have to exlude bracket characters by [^\\]]* which means everything except ], [ can be repeated unlimited in the group 1 to avoid selecting something lik [anv](anv.org) blah blah [bcd](bcd.org). \\[\\[([^\\]]*)\\]\\]\n",
    "ref": "/blog/blogposts/regex/"
  },{
    "title": "Journal: Announcements about my blogging activity",
    "date": "",
    "description": "",
    "body": "Some Announcement I am back at writing again. Don\u0026rsquo;t get me wrong, I still write everyday, but for myself, and it\u0026rsquo;s been a long time that I have not written anything publicly. Well, that\u0026rsquo;s about to change since I have some ideas to write about frequently. The theme of my frequent writings are the followings:\nResearch Paper Summaries Journals and Diary Book Summaries and Recommendations Research Paper Summaries The papers are going to be mostly in Machine Learning and Artificial Intelligence, but I might write about other fields occasionally; these other areas could be Neuroscience, Cognitive Science, and more. These posts and summaries are going to be technical. However, I plan to write non-technical posts about some of these papers or about an idea that is inspired by these papers and connecting them to other ideas. The non-technical posts will either end up as a section in journals (I have to find a better name for them; how about primal?) or in a topic dedicated to that idea as a its own separate post.\nThe Hierarchy There will be a hub page which lists all papers that I summarize. This page acts as a control center or a hub if you may. I provide very short summaries for every paper here (about 2 sentences). The detailed summaries will be in a different post dedicated to each paper. However, not all papers in this page are going to be summarized in detail. Therefore, if you see one paper has a link that you can click on, it means I have summarized that paper in more depth, otherwise the short summary you see in the hub page is everything that I have written about that paper so far. Note that the status of papers may change as I might read them in more depth in the future.\nThe Structure For detailed summaries, I will have a fixed structure to facilitate reading for you. This structure is also changing over time as I gain more experience and hear your feedbacks hopefully! As of right now the structure includes the following sections:\nUp to five keywords A summary consisting of about 200-300 words Terminology My thoughts on the paper Key takeaways, or rather a summary in the form of bullet points Journals These kind of posts will usually be short and all over the place. The title will be just the date. The purpose of these posts is to share the the material(s), resource(s), and the thoughts that I have about a subject as quickly as possible without getting too deep in the matter, and without thinking about a title for it. If I start to think of a title, then that post should be about one concept only, and if it\u0026rsquo;s only a few sentences, I would not publish it until I make it perfect. However, if I write whatever I have in mind as a diary/journal post, then at least I will do it. Right now I\u0026rsquo;m using the date of publication as the tile for this type of posts. But I am thinking of having 1 diary/journal post per year, and edit the same post and add a new section whenever I want to write a new entry. Please let me know which one is more useful; new posts per journal, or one post per year that gets updated?\nBook Summaries ",
    "ref": "/blog/blogposts/journal-announcement-2021-11-03/"
  },{
    "title": "Note Taking",
    "date": "",
    "description": "",
    "body": "Hey You Perfectionist Don\u0026rsquo;t spend endless hours contemplating the tools. Follow this rule:\nAs long as you have access to your data and they are palin text, you will be fine. The worst is that you can\u0026rsquo;t convert your files easily to another plain text format, but even in that case you can search your files and find the data you are looking for. Don\u0026rsquo;t think about publishing and making it look good, because notes evolve over time and you don\u0026rsquo;t want the pdfs of them you created two years ago. The important part is the raw information you capture and not how it looks. You can always take care of that later. In terms of converting formats: Even if you can\u0026rsquo;t use automatic tools, you can use regexp to convert the syntax more or less. Don\u0026rsquo;t think about folders and hierarchies. You can always change them relatively easily. So, if you have a tool that\u0026rsquo;s working for you now, just f***ing USE it and don\u0026rsquo;t think about the future of the tool and what happens to it and to your future self. TLDR Editor: I use Emacs for note taking and Vscode for coding.\nLanguage: I decided to use Org Mode inside emacs for a few months now and I find it extremely helpful.\nWhy Notes aren\u0026rsquo;t a record of my thinking process. They are my thinking process. \u0026ndash; Richard Feynman\nThe primary purpose of note-taking should not be for storing ideas, but for developing them. When we take notes, we should ask \u0026ldquo;in what context do I want to see this note again?\u0026rdquo; All the time I spend configuring and debugging Emacs is time I\u0026rsquo;m not spending doing something more productive, Restaurant kitchens are not pretty because professionals care about efficiency and not pretty. Result I will use vscode/vscodium as my main eidtor for now so that I can get rid of Obsidian. Then if I don\u0026rsquo;t like it completely, I move to emacs. Use a combination of vscode, emacs, and obsidian. You can use obsidian to print pdfs. Use vscode for snippets, multi-cursor, export to latex, and its huge extension market which is awesome for getting things done and project management (the closest to org-mode I can get). Emacs is used to do stuff that I can\u0026rsquo;t really do here. But make sure to use a universal syntax so that you can use vscode or emacs later if something happens to obsidian. In the meantime vs code extensions will improve and new apps will come out. As long as the syntax is the same, I will be fine. The only things that are not standard markdown are:\nembedding notes wikilinks (which luckily all three editors support). I can use wikilinks for every file format in both vscode and obsidian. So I guess it makes sense to continue using it. Because if I ever go to emacs, I will be using org mode for sure and markdown-mode in emacs is not very powerful (no link autocompletion, backlinks, \u0026hellip;) link to sections. So bottom line: use either of these tools but don\u0026rsquo;t link to section and don\u0026rsquo;t embed files very often (I\u0026rsquo;m sayin very often because doing so doesn\u0026rsquo;t break the links and a simple link will be added to the file).\nI also can download the installation of the current version of obsidian which is free so that if they changed it, I can have a working base.\nI am very efficient with obsidian and it has all of the features I want. The only problem is that they might change it to require subscription. I will continue to learn emacs org mode and org roam but it\u0026rsquo;s too much time sink and prevents the original deep work. I don\u0026rsquo;t think vscode extensions are there yet to be used as my primary tool for second-brain-zettlekasten. I will check vscode extensions again if Obsidian discontinues the free version. Also, all my notes are in markdown, and converting them to org won\u0026rsquo;t be easy.\nMemo extension in vscode is almost identical to obsidian, so I can easily write in the same format in both. The only difference is linking to sections which is not supported in vscode. The other problem with vscode is the preview/export to pdf which is not as powerful as obsidian. The preview/export feature is not a deal breaker right now because it doesn\u0026rsquo;t affect my method of note taking and writing. I think I can make a css stylesheet to export to html based on it.\nMethods Outline: Markdown, org mode Zettelkasten Mind Map Cornell: Notebooks and papers Sentence Charting Techniques Separate facts from your own opinions Use/Create a set of symbols to make note taking easier w/ instead of with and w/o instead of without. Use numbers instead of letters. 3 instead of three. Use a symbol like \u0026ndash;\u0026gt; to replace if-then statements Language of Notes Markdown Use mermaid to draw graphs in your markdown note. The following diagram is to decide which language to use\ngraph TD; A[org mode] --\u0026gt; |No| Latex --\u0026gt; No; A --\u0026gt; |No| M[Markdown]; A --\u0026gt; |Yes| Emacs; M --\u0026gt; Apps --\u0026gt; Zettlr; Apps --\u0026gt; Obsidian; M --\u0026gt; T[Editors] --\u0026gt; Emacs; Emacs --\u0026gt; Doom; Emacs --\u0026gt; Spacemacs; Emacs --\u0026gt; GNU; T[Editors] --\u0026gt; V[VS Code] --\u0026gt; Foam; V --\u0026gt; Dendron; V --\u0026gt; Memo; T[Editors] --\u0026gt; vim; Org Mode See Org Mode for a comprehensive instruction and to see what\u0026rsquo;s possible with org mode.\nMarkdown or Org mode Read this: http://www.pwills.com/post/2019/09/24/blogging-in-org.html\nOrg mode workflow by org roam\u0026rsquo;s author https://blog.jethro.dev/posts/org%5C%5Fmode%5C%5Fworkflow%5C%5Fpreview/\nFeature Org (WYSIWYG) Markdown open linked files in editor ✓ ✓ open linked files in html ✓ open linked files in pdf ✗ ✗ embeding files (transclusion) ✓ ✓ export to latex ✓ ✗ Equation numbering ✓ ✗ Diagram/Merimaid ? ✓ Tags headlines and metadata ✓ More apps ✗ ✓ Consistent syntax ✓ ✗ Org mode means I have to use emacs for the rest of my life since there are no better editors for org mode. Markdown is more widely used and there are more apps for it but it\u0026rsquo;s less powerful than org (equation numbering, export to latex) and there is no standard markdown way of writing (wkilinks vs regular links). DONE I think org mode is out of question for me, since I already have a lot of markdown notes.+ Convert markdown files to org mode files. I converted my files to org mode and I\u0026rsquo;m enojoying it so far.\nI feel I\u0026rsquo;m not ready to make a commitment to emacs and use it for my note taking and everything, because I\u0026rsquo;m not proefficient in it yet. Also converting all of my markdown notes to org mode is a pain. Maybe the best choice is to use org mode for paper summaries so that I can get used to it and also learn how to make adjustments to emacs with elisp. Then after a while I can decide better because I have more data/knowledge to reason with.\nSome references:\nhttps://cheatsheet.md/notable.pdf https://karl-voit.at/2017/09/23/orgmode-as-markup-only/ http://doc.norang.ca/org-mode.html Exporting graph TD; Markdown --\u0026gt; HTML --\u0026gt; PDF ; Org --\u0026gt; Markdown; Org --\u0026gt; LaTeX --\u0026gt; PDF; Org --\u0026gt; HTML; Convert Markdown to org mode resource\nChange wikilinks to regular markdown links Use vs code\u0026rsquo;s find and replace with regex to change links from [some-link](some-link.org) to [some-link](some-link.org). Use a regexp similar to below to find wikilinks\n\\[\\[([^\\]]*)\\]\\] and replace them by\n`[$1]($1.org)` Add .org extension to the regular links but ignore other file formats and sections Then some of them are wikilinks to sections which contain # so just ignore those with regex [^#] and some of them are wikilinks to othe file formats wich contain dots in the parantheses so ignore those as well using regex [^.] or altogether [^#.]. Next you can convert [some-link](some-link.org) to [some-link](some-link.org) Basically, you have to swap \\[([^\\]]*)\\]\\(([^\\]\\)#.]*)\\) with [$1]($2.org)\nMove all markdown files to a single directory one directory find . -name \\*.md -type f -exec mv {} ../all_knowledge \\; Convert all the files from markdown to org $ find . -name \\*.md -type f -exec pandoc -f markdown -t org -o {}.org {} \\; #DONE the script above convert the files but gives them the extension of .md.org while I want .org only so that I can have consistent links. Use the following script instead.\nfor md in $(find . -name \\*.md -type f) do echo ${md} # echo \u0026#34;${md%.*}\u0026#34; pandoc --wrap=preserve -f markdown -t org -o ${md%.*}.org ${md}; done To convert only one file use\npandoc -f markdown -t org -o newfile.org original-file.markdown Remove custom\\_id properties created by Pandoc for each sections Use vscode\u0026rsquo;s find and replace with regex to remove them easily by\n:PROPERTIES:\\n.*:CUSTOM_ID:.*\\n.*:END: Convert tags to org headlines with keywords. Again, use vscode\u0026rsquo;s find and replace with regex to remove them easily by the following.\n.* \\[( |x)\\].*#(TODO|DONE|HOLD|KILL) and replace it by\n*** $2 Note that if a tag is not present in your emacs org-todo-keywords, then you need to convert that one separately so that you can map it to a specific keyword.\nAdd title, author, and org-roam keys Use the following bash script:\nfor org in $(find . -name \\*.org -type f) do name=$(basename ${org%.*}) # Remove extension and path to file title=$(echo $name | tr \u0026#34;-\u0026#34; \u0026#34; \u0026#34;) # Replace dashes with whitespaces title=$(echo $title | python3 -c \u0026#34;import sys; print(sys.stdin.read().title())\u0026#34;) # Capitalize first letters # echo \u0026#34;#+TITLE: \u0026#34;${title} (echo \u0026#34;#+ROAM_KEY: \u0026#34;${name} \u0026amp;\u0026amp; cat $org) \u0026gt; filename1 \u0026amp;\u0026amp; mv filename1 $org # Third line of org file (echo \u0026#34;#+AUTHOR: Kasra Darvish\u0026#34; \u0026amp;\u0026amp; cat $org) \u0026gt; filename1 \u0026amp;\u0026amp; mv filename1 $org # Second line of org file (echo \u0026#34;#+TITLE: \u0026#34;${title} \u0026amp;\u0026amp; cat $org) \u0026gt; filename1 \u0026amp;\u0026amp; mv filename1 $org # First line of org file done Now you can convert links to section to its correct format in org Use vscode\u0026rsquo;s find and replace tool to do that, or simply modify them while you encounter them. swap the \\[\\[file:([^\\]]*)#([^\\]]*)\\]\\[([^\\]\\)]*)#([^\\]]*)\\]\\] with [[file:$1.org][$3]].\nTo convert all the files use:\n$ find . -name \\*.md -type f -exec pandoc -f markdown -t org -o {}.org {} \\; KILL Converting markdown to org mode with this syntax doesn\u0026rsquo;t convert links at all and all of them are broken. First, I wanted to use the following script but there were complexities including having multiple files of the same name. Anyway, I came up with an idea: I can move all my files to a single folder. Then I can use regex to change the wikilinks to markdown links and after that adding org extension to the ones that don\u0026rsquo;t have any extension. Next, using Pandoc I can convert my files to org-mode. Finally using dired and org-roam in emacs I can easily move files to corresponding folders again.\nfor f in all_markdown_files: for wikilink a [a](a.org) in f: find a in all_files newlink = os.getcwd() +\u0026#39;/\u0026#39; + a + \u0026#39;.org\u0026#39; replace wikilink with newlink DONE the script above convert the files but gives them the extension of .md.org while I want .org only so that I can have consistent links. Use the following script instead. for md in $(find . -name \\*.md -type f) do echo ${md} # echo \u0026#34;${md%.*}\u0026#34; pandoc -f markdown -t org -o ${md%.*}.org ${md}; done To convert only one file use pandoc -f markdown -t org -o newfile.org original-file.markdown\nText Editors Refer to Editors War for more info.\nThe result of the table below clearly suggests that I should be using vs code at least for markdown. It doesn\u0026rsquo;t make sense to use emacs for markdown when I already have everything I want in vscode. It\u0026rsquo;s clearly better than obsidian since it\u0026rsquo;s open source.\nFeature/App Obsidian VS Code Emacs Markdown Emacs Org Language Related Wiki links ✓ ✓ ✓ ✓ Link to other file types ✓ ✓ ✓ B. ✓ link to sections ✓ ✗ ? ✓ Search files to link ✓ ✓ ✗ A. ✗ A. Embedding notes ✓ ✓ ✗ ✓ Tags ✓ ✓ ? ✓ LaTeX ✓ ✓ ✓ ✓ LateX: Equation numbering ✗ ✗ ✗ ✓ Mermaid/Diagram ✓ ✓ have to install ✓ C. Citation/References ✓ Footnotes ✓ ✓ ✓ Editor Related - - - - Free, Open Source ✗ ✓ ✓ ✓ Snippets ✓ ✓ ✓ ✓ Open daily notes w/ template ✓ ? ? ? Own your files ✓ ✓ ✓ ✓ Backlinks ✓ ✓ ✗ ✓ Fuzzy File Switch/Creation ✓ ✓ ✓ ✓ Preview/Export to pdf pdf pdf E. html latex,pdf,html Automatic link renaming ✓ ✓ ✗ ✓ roam Outline ✓ ✓ ✓ ✓ Mind map ✓ ✓ ? ✓ Automatic table formatting ✓ ✓ ✓ ✓ Daily notes ✓ ✓ ? ? Knowledge graph ✓ ✓ ? ✓ Fold/unfold ✓ ✓ ✓ ✓ Not very important - - - - Multi cursors ✗ ✓ ✓ ✓ Fast ✓ ✓ ✓ ✓ Custom CSS ✓ ✓ ? ? powerful text editor ✗ ✓ ✓ ✓ Easy ✓ ✓ ✓ ? Beautiful ✓ ? ? ? Emacs org can search for files to link in one directory only, but not in subdirectories which is not good. In Markdown you can\u0026rsquo;t even choose files in the current directory and you have to write the whole thing.\nYou should use the format [text](link.org). The problem is opening them either in the editor (if supported such as image, pdf) or in system\u0026rsquo;s default app. In emacs, zettlr, and probably other markdown editors I can use [wikilinks](wikilinks.org) only for .md files and not other types. For other types, I have to use [text](link.org). In vscode and obsidian though, I can use wikilinks for other file types as well. So I guess it\u0026rsquo;s better to follow emacs paradigm since it\u0026rsquo;s the standard markdown, and probably vscode and obsidian are redesigned to handle this which is not a future-proof feature. The problem is that in vscode I can only open other formats in their default app if they are linked using [wikilinks](wikilinks.org).\nI have to save it to a file first as a png. So basically ob-mermaid embeds an image of mermaid in the exported file which is not good quality. You can easily use a pdf file so that you don\u0026rsquo;t lose any quality.\nIt doesn\u0026rsquo;t include embedded files in enhanced preview, and in the pdf. You only need pdf for your paper summaries because other notes are constantly changing, and in your summaries you don\u0026rsquo;t need to embed anything.\nI have 4 choices:\nA dedicated app such as obsidian. Chances are they become proprietary and require subscription but they are usually easy to use. As long as you use a universal syntax, it\u0026rsquo;s fine. A modern text editor such as vscode. These are usualy based on web apps and JS so they can show different file formats and are powerful enough but you are not 100% in control here. A full text editor such as emacs or vim. You can be sure you always have them but they are not as good looking as the other two options I can use a combination of apps. For example I can use markdown and write in emacs, and when I want to see the backlinks, I can use vscode. If you want to have the most power, then emacs can be tweaked completely while vs code is not as configurable as emacs since it is for microsoft. The easiest and most beautiful: obsidian The most configurable, powerful, and hardest: emacs\nvscode and Sublime Text use plugins in a more traditional way. E.g. in Atom packages can change how editor works, while in Sublime Text and VS Code plugins mostly add features on top of how editor works.\nObsidian /blog/blogposts/obsidian/ Everything is great, but something like org-agenda is missing. Also not as powerful as emacs or vscode when it comes to being a text editor.\nEmacs. Org mode. Org roam /blog/blogposts/emacs/ I feel like markdown in emacs is not going to be easy and I have to spend a lot of time finding/writing tools that are already available in vscode and obsidian. Zettelkasten note taking in markdown in emacs: https://www.eliasstorms.net/zetteldeft/\nOrg-mode Workflow Part 3. Zettelkasten with Org-mode by the author of org roam: https://blog.jethro.dev/posts/zettelkasten%5C%5Fwith%5C%5Forg/\nVS Code and extensions /blog/blogposts/vscode/\nVS Code can easily replace obsidian since it provides all the features and is a more powerful text editor such as:\nmoving list items up and down updates the item\u0026rsquo;s number while Obsidian can\u0026rsquo;t do that. when clicking on a todo, it opens the exact line in the corresponding file while obsidian sucks at that. Snippets and templates are more powerful and I can have placeholders and use tab to move among them when I insert a template. The Problems headings are not bigger than body text. It can\u0026rsquo;t be fixed apparently. Preview/Exporting is not very good both in terms of beauty, custom CSS, and exporting to pdf easily. Opening the monthly note with a keybinding or a command. I might be able to modify memo for this one since it has daily notes. Creating a new note and searching for existing note at the same time. I guess one of the extensions can handle this. Yes. you can use VSnotes for that. It seems none of the extensions are as complete as Obsidian. But I might be able to modify/use multiple extensions to make it happen. Check out the following link to get some idea for doing this. https://hodgkins.io/vscode-second-brain Zettlr Vim No. It\u0026rsquo;s not as good as obsidian and emacs. Knowledge graph is not available, org-mode and org-agenda are not available either. Basically it\u0026rsquo;s only a text editor and it can\u0026rsquo;t show any graphical stuff such as pdfs and inline rendered LaTeX. Also sometimes you should go with your guts, and my gut says Richard Stallman.\nThe only thing lures me is that vim is faster and easier (both to learn and modify), but if you want easy, then why note vs code?\nOther apps Typora EverNote Onenote Notable Roam Research Notational Velocity: No. Its UI is terrible and not user friendly at all. Tags are not supported nvALT: No. Same as Notational Velocity, it doesn\u0026rsquo;t autocomplete the wikilinks and tags are not supported. FS Notes: No. Tags don\u0026rsquo;t show up quickly. It doesn\u0026rsquo;t have any awesome feature that Obsidian doesn\u0026rsquo;t already have. Traditional Notebooks Notion.so No. It\u0026rsquo;s proprietary. I cannot navigate with keyboard only among tables I can replicate the feature that every entry of a table is a note for itself here in Markdown and Obsidian just by linking to a note.\nPros\nBeautiful UI Calendars to select data Databse approach is good where every entry can be opened separately You can select a type for each cell of the table and then you can only add the relevant values for that cell Spreadsheet like tables where you can compute avg, sum and other functions. Cons\nVendor lock-in: I don\u0026rsquo;t own/control my data 5 MB limit for file attachments Universal search function is not good and doesn\u0026rsquo;t find what I want Not easy to navigate since it\u0026rsquo;s not completely a text editor and you need to use your mouse eventually at some point For example when you are in a table, you can\u0026rsquo;t go to the heading of that page with keyboard Mind Map use draw.io to draw mind maps and use this link to boost your speed when creating mind maps in draw.io\nHandwrite There are couple of options but remember that you don\u0026rsquo;t have access to files in any of these applications because the format is proprietary and they can only be opened in ther dedicated apps and the best yo ucan get at least for now is link sharing.\nSince you cannot git control these files, it really doesn\u0026rsquo;t matter. You can\u0026rsquo;t back them up because google products like jamboard are not files, and apple doesn\u0026rsquo;t show you the files, but I beleive you can backup microsoft products.\nYou can use Apple for personal stuff, and Microsoft and Google for research and education.\nMicrosoft OneNote Link: No. You can\u0026rsquo;t link to a specific page or section. You can only link to a notebook. Speed: No. It\u0026rsquo;s very slow to load. Tool: Yes Apple Notes Link: Yes Speed: Crashes Tool: No If it doesn\u0026rsquo;t crash, I will use it.\nGoogle Jamboard Use this for collaboration only when you need a whiteboard.\nLink: Yes, you have to create a new file everytime. Speed: Yes Tool: No NO Google Keep You don\u0026rsquo;t have access to files to be able to link to them.\nNO Random Free apps on iPad They usually don\u0026rsquo;t let me to store files on my drive.\nResources list of apps: https://github.com/prathyvsh/networked-notebooks list of apps: https://danmackinlay.name/notebook/note%5Ftaking.html A Writing Workflow: https://keleshev.com/my-book-writing-setup/ Migrating from Obsidian to emacs: https://notes.huy.rocks/emacs-for-note-taking Comparison of vscode and emacs: https://www.admiralbumblebee.com/programming/2020/01/04/Six-months-VS-Code.html#scripting A good repo about people, articles, and apps for personal knowledge management: https://github.com/brettkromkamp/awesome-knowledge-management ",
    "ref": "/blog/blogposts/note-taking/"
  },{
    "title": "Blogging",
    "date": "",
    "description": "",
    "body": "Tools Jekyll Edit files in BuildBlog folder. Do the following in terminal\ncd BuildBlog export PATH=$HOME/.gem/ruby/2.6.0/bin:$PATH JEKYLL_ENV=production bundle exec jekyll build Then copy the \\_site contents to blog folder in your github repo.\nYou should do this every time you add a new post.\nHugo Create the website (Once) hugo new site blog_name cd blog_name Add a new post hugo new posts/my-first-post.md Publish The Blog hugo server -D # To see it live hugo -D # To build the website Output will be in ./public/ directory by default (-d/\u0026ndash;destination flag to change it, or set publishdir in the config file). Copy the output to github.\nDesign UI/UX Users are more likely to scroll all the way down than to click on the next page button since it requires another loading time and changing the method of using keyboard/mouse. Therefore, it\u0026rsquo;s better to have all of your posts in one page, and load them incrementally.\nFont Use a comic like font such as the font of this blog\n",
    "ref": "/blog/blogposts/blogging/"
  },{
    "title": "About Kasra Darvish",
    "date": "",
    "description": "",
    "body": "Who Am I what I do? Am I what I like? Am I just my name? Am I my income? Am I the number of people who love me?\nMaybe, but none of them describe me completely. I\u0026rsquo;m defined by the moments that I laughed my a** off, the moments that I cried so hard, the moments that I made someone smile, the moments that I helped someone, the moments that I needed to be helped, the moments that I loved others, the moments that I learned something. You see, it\u0026rsquo;s almost impossible to introduce someone in few sentences, you have to live with them for a while to know them. This blog is a small window to my life and I hope I can help you learn something, or just have a good time reading.\nMy main interests are intelligence and decision making. More specifically, I\u0026rsquo;m interested in Artificial Intelligence, Machine Learning, Robotics, Brain/Mind, Psychology, and Sociology. You can also take a look at my resume here.\nMy hobbies are piano, soccer, playing logic/reasoning games with my friends, and reading books.\nWhat I write about life, intelligence, finance, investment, artificial intelligence, machine learning, and books. I\u0026rsquo;m happy that you decided to join me to walk through this adventure.\nSupport I\u0026rsquo;ve never liked it when I was reading a blog post and it asked me to subscribe in the middle of the post. However, since I started to write this blog I see that it\u0026rsquo;s hard to continue without any financial supports, but I still want to keep my content freely available because I believe that knowledge is priceless.\nThat being told you can support and encourage me by buying me a coffee using the yellow button below which I appreciate it in advance!\n",
    "ref": "/blog/about/"
  },{
    "title": "How to apply",
    "date": "",
    "description": "How to apply for graduate studies in USA",
    "body": "In this post, I am going to talk about the process of applying for universities in United States of America, but most of the parts are general and applicable to other countries as well. I am assuming that you are an undergrad or Master\u0026rsquo;s student and want to apply for Master\u0026rsquo;s or Ph.D. degree. Before starting to apply for universities, you need to have a reasonable resume, which means you have to learn different skills and have some research experiences in your field of study. Moreover, a good GPA is always important, so if you are a first year student, try to get good grades in your exams. If you don\u0026rsquo;t want to apply, there is no need to have a high GPA, and you can spend your time to learn different things or have fun or whatever you want, but since you are reading this post, you are thinking about applying for grad school, therefore you need your GPA.\nThe first step is improving your English skills (if you are from a country in which English is not the official or academic language). These skills are reading, listening, speaking, and writing. You should also expand your vocabulary and learn the grammar (which is not very difficult). I will discuss more about how to improve your English in another post separately.\nThe second step is taking the English exam (TOEFL, IELTS). If you want, I can write about the differences between the two exam, and which one is better to take for different people. For TOEFL, scores more than 100 are good, and for IELTS scores more than 7 and 7.5 are good. It varies based on your field of study.\nThe third step is taking the GRE exam and you need to improve three skills: Quantitative Reasoning, Verbal Reasoning, and Analytical Writing.\nThe forth step is to start looking for universities and professors, and decide about your research interests. You can use some ranking websites, but use them only for finding universities, not to compare them, since these rankings are not realistic. You can use QS or USNews. Computer Science students can use this perfect website. In each university find a professor whose research interests are the same as yours, and send an email to them. (Read How to send an email to a professor?). The next step is to write your SOP (Statement of Purpose) which is usually 1 page or 2 pages long. SOP is one of the most important parts of your application and you must put a lot of effort to write a good SOP. In SOP you should write about yourself, your passion, why did you study in this major, why you have these research interests, and a most important question, \u0026ldquo;why do you want to attend to grad school as a Master\u0026rsquo;s or Ph.D. student, and why this university\u0026rdquo;. The last step is to decide what universities you want to apply, and start filling out application forms.\nTo sum it up, the documents you need to apply for grad schools in the USA are, transcripts, official TOEFL/IELTS score, official GRE score, resume, SOP, up to 3 recommendation letters. I wish you the best of luck!\nP.S. I wrote this post in English because if you want to apply for a university in the USA, you have to be able to read an article of this length and difficulty easily :)\n",
    "ref": "/blog/blogposts/2018-11-21-how-to-apply/"
  },{
    "title": "",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/blog/blogposts/editors-war/"
  },{
    "title": "",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/blog/blogposts/obsidian/"
  },{
    "title": "",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/blog/blogposts/second-brain-zettlekasten/"
  },{
    "title": "",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/blog/blogposts/vim/"
  },{
    "title": "",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/blog/blogposts/vscode/"
  },{
    "title": "Channelb",
    "date": "",
    "description": "",
    "body": "Type: #podcast\nScientist and the Sea (Episode 60) status: #DONE The story of Tommy Gregory Thompson a scientist who turns to a thief. He is known for his leading role in the rediscovery of SS Central America on September 11, 1988.\nSilk Road Keywords: Tor Browser, Dark Web, Drugs, Libertarian\nThe story of Ross Ulbrik who created a dark net webiste to sell drugs. He was a proponent of libertarian and freedom and that was why he started this website.\nWhy this podcast Good Storytelling but long. The same story can be told in half that time. Informative about real life events Good as a hobby and not a source of info since it\u0026rsquo;s not fast enough Episodes Name Status Genre Date Rating Duration channelB #DONE #documentary 2021-04-14 4/5 66 ",
    "ref": "/blog/blogposts/channelb/"
  },{
    "title": "Emacs",
    "date": "",
    "description": "",
    "body": "Pros org mode extensible Powerful text editing capabilities Org agenda is really good Future proof Cons Emacs is single-threaded so a buggy plugin can freeze your emacs process. Not very fast Learn To search and open a file in all subdirectories from the current directory use Find file from here command or spc+f+F.\nTODO how to open files in system\u0026rsquo;s default app form emacs? General Packagaes wc-mode to give you number of words in the file. writeroom-mode for zen mode writing. Web browser in emacs: https://www.emacswiki.org/emacs/CategoryWebBrowser Coding Features Packages Org-mode take notes in org with sacha\nOrg-mode Packages mind map or outline similar to obsidian Paper Reading and Literature Review Zotero \u0026ndash;\u0026gt; org-roam-bibtex \u0026ndash;\u0026gt; org-roam \u0026ndash;\u0026gt; org-mode use the following packages for research in emacs:\norg-noter, org-noter-create-skeleton: annotate pdfs org-capture, pdf-tools, org-roam-server, org-agenda, org-super-agenda: for habbit tracking Org-roam The org-roam-db (database) is located at ~/.doom-emacs/.local/opt/org-roam.db if you want to use org-roam version 1, you can simply delete the database. close the emacs. Open it again and run M-x org-roam-db-build-cache. If in your init file you had org +roam2, you need to change it to +roam and run doom sync for changes to take effect.\nMarkdown https://jblevins.org/projects/markdown-mode/ or https://github.com/jrblevin/markdown-mode\nPackagaes Preview https://wikemacs.org/wiki/Markdown#Live%5C%5Fpreview%5C%5Fas%5C%5Fyou%5C%5Ftype https://stackoverflow.com/questions/8828853/is-there-a-way-to-look-at-a-rendered-website-from-within-emacs https://stackoverflow.com/questions/36183071/how-can-i-preview-markdown-in-emacs-in-real-time/36189456\ngrip-mode: It provides a really good live preview in a web broswer. I prefer it to be in the emacs. The problem is that it\u0026rsquo;s limited to some amount of request per hour since it\u0026rsquo;s using github. gh-md: No live update Doom Emacs But before you doom yourself, here are some things you should know:\nDon\u0026rsquo;t forget to run \u0026lsquo;doom sync\u0026rsquo;, then restart Emacs, after modifying ~/.doom.d/init.el or ~/.doom.d/packages.el. This command ensures needed packages are installed, orphaned packages are removed, and your autoloads/cache files are up to date. When in doubt, run \u0026lsquo;doom sync\u0026rsquo;! If something goes wrong, run doom doctor. It diagnoses common issues with your environment and setup, and may offer clues about what is wrong. Use \u0026lsquo;doom upgrade\u0026rsquo; to update Doom. Doing it any other way will require additional steps. Run \u0026lsquo;doom help upgrade\u0026rsquo; to understand those extra steps. Access Doom\u0026rsquo;s documentation from within Emacs via \u0026lsquo;SPC h d h\u0026rsquo; or \u0026lsquo;C-h d h\u0026rsquo; (or \u0026lsquo;M-x doom/help\u0026rsquo;) Have fun!\nReferences why emacs is better and Doom Emacs config ",
    "ref": "/blog/blogposts/emacs/"
  },{
    "title": "Org Mode",
    "date": "",
    "description": "",
    "body": "Links Insert a link: C-c C-l and then choose org: for example Edit a link: Same as inserting a link To link to a url use the following syntax: [[LINK][DESCRIPTION]] Link to other file formats: You can simply use the syntax above for urls, but instead of a url, provide the path to a file. You can alternatively use the command org-insert-link or keyborad shortcut spc m l l. The default keybinding is C-c C-l. Example: code or audio. Use counsel to insert internal links to headings in the current file/buffer. Autcompleting the links simply by ?? I don\u0026rsquo;t know yet. Can I link to a markdown from org and have it backlinked? I can link to whatever file I want, but it will break the knowledge graph. Let\u0026rsquo;s write something item 1 in order to have a bullet point you can hit space and then * on a new line like this Headings and Folding create a new heading/lit item of the same level by using cmd+enter or M+ret read it as alt+enter create a new sub heading by using Folding use shift+tab to fold/unfold all headings at once use tab to fold/unfold the heading under the cursor using M-x clone-indirect-buffer you can have an indirect buffer that is a twin copy of the current buffer but does not update with the other one. It\u0026rsquo;s very useful to have an outline for a file Level 3 Level 4\nLevel 5\nLevel 6\nLevel 7\nLevel 8 Focus mode or narrowing You can focus a buffer on a subtree by using C-x n s or the command (org-narrow-to-subtree) which narrows the current buffer to the subtree at point. You can widen the buffer back or remove narrowing by C-x n w\nText Annotations underline italic bold code verbatim stirke :tag: Embed or include org files in another org file (transclusion) Why Emacs? Many platforms have a single item clipboard. Emacs has an infinite clipboard. If you undo a change, and then continue editing, you can\u0026rsquo;t redo the original change. Emacs allows undoing to any historical state, even allowing tree-based exploration of history. Emacs supports a reverse variable search: you can find variables with a given value. You can perform structural editing of code, allowing you to make changes without breaking syntax. This works for lisps (paredit) and non-lisps (smartparens). Many applications use a modal GUI: for example, you can\u0026rsquo;t do other edits during a find-and-replace operation. Emacs provides recursive editing that allow you to suspend what you\u0026rsquo;re currently doing, perform other edits, then continue the original task.\nSome keybindings and commands spc ? shows all the key bindings spc : or M-x for typing your command escp goes to command mode. i goes to insert mode cmd-f activates swiper C-s activates i-search and after entering your term, you can use C-s to search for next occurences tramp for ssh multiple cursor doesn\u0026rsquo;t work or I don\u0026rsquo;t know yet cmd-l goes to line cmd-\u0026rsquo; changes windows cmd-w close a window cmd-t creates new workspace/tab/sessions and you can navigate them with M-1,2,3 each number is one of the workspaces select a region and C-x tab for indention. then use H and L horizontal split spc-w-s vertical split spc-w-v In emacs C-x generally refer to global commands, while C-c refers to language specific commands. Help You can see the current value of a variable, any variable, by using the describe-variable function, which is usually invoked by typing C-h v Themes I am using chemacs which is a config profile switcher that allows me to select from different configs https://github.com/plexus/chemacs use the following command to choose other configs: emacs \u0026ndash;with-profile prime\nFiles spc f for anything related to files spc spc for opening files in all subdirectories. spc f f or spc . to open directory to select or create a file spc f r shows a list of recent files (files recent) spc f p to open config files (packages, init, config) spc f l to locate a file just by its name. Much like cmd-o in Obsidian. Navigation alt and left/right arrows: jump over 1 word. alt and up/down arrows: swap two lines. fn and left/right arrows: jump to the beginning or the end of the file. Text Edit copy/cut/paste cut a peice of text: You can just delete the piece you want and it will be cut but only using emacs keys. Amazing! If you want to select a region/text and then cut it: C-spc to start highlighting, and then press cmd-x or d to cut. M-del or C-del to cut the previous word. M-d for cutting the next word. cmd-c to copy a text cmd-v to paste a text To edit multiple items at once, select the word first and then hit R to select all occurences. Then either add characters or to completely change it first use C-del or M-del to delete sected word and then start typing (isntead of selecting it and then start typing) Mark/highlight region/text In doom emacs you could use C-M-SPC to select text. In regular emacs you could use C-SPC to do that. You can then add enclosing parantheses or other things.\nTricks Read this Nifty Tricks.\nuse c s ( [ to change a matching parantheses to a matching bracket. to change inside a matching parantheses, use f ( l to find the starting \u0026lsquo;(\u0026rsquo; and then move one character so that you are inside it. Then use er/expand region command to select evreything inside. Then use d to cut and i to go to insert mode and start writing. Uppercase/lowercase, capitalize Select a region by C-M-SPC and then run the command upcase-word or upcase-region to make a region all uppercase letter. Do the same and run the command downcase-word or downcase-region to make a region all lowercase letter. Column Editing C-x SPC down down C-x r t insert-what-you-want RET Buffer spc b for anything related to buffers. spc b ] for next buffer and [ for previous buffer spc b B to list all buffers spc b b to list workspace buffers (Doesn\u0026rsquo;t list messages, scratch, other unnecessary stuff) spc b k kills the current buffer spc b i to list all buffers in a window instead of promp so that you can either open them or kill multiple of them at once. To delete buffers from the list mention in previous item, just press d on each line you want to kill. Then afte ryou selected all of the buffers you want to delete, press x. Press u for undo selection on each line. Integrated Terminal In order to open a file from an integrated terminal such as vterm you can use emacsclient and provide -n flag which means no-wait.\nemacsclient -n file.txt You can then close the terminal if you want and the file stays alive.\nPython: code completion multiple cursors Keybindings use C-h b (help binding) in order to see all of the keybindings in the current buffer. This is useful when you find some keybindings for a task, but you want to remap it, so you can search the keybindings to find it and then remap the command. Keys use C-h k and then hit the key on your keyboard for which you want to see the description.\nTramp to terminate or abort a running command use C-c C-c to open vterm at current file (whether local or over ssh) use +vterm/toglle command or M-spc o t DONE make tramp work with vterm, the regular shell is not good enough. I can ssh to server separately with vterm, but I don\u0026rsquo;t like it. I want to be able to open and close terminal whenever I want. DONE make tramp work with screen. This might be solved if I can use vterm. org mode + roam press tab to fold/unfold subtress and shift-tab to fold/unfold the whole document C- creates a new item of the same level (heading, item, or \u0026hellip;) even if you are on a heading and there are bunch of stuff below it, this key cord makes a new heading after all of the stuff M-l makes an item 1 level lower, similarly M-h for higher Check this file for more details: basics of org mode How to write latex and code in org mode? Isn\u0026rsquo;t it easier to use markdown for note taking? To move one line/section up or down use shift-alt-up and shift-alt-down respectively KILL this is my task Multiple Cursors Let\u0026rsquo;s say I want to change all my obsidian references to emacs.\nObsidian is too good and easy to be true. My text editor is obsidian.\nGreat packages Magit: the best git client in the known universe expand-region: projectile: powerful project navigation/interaction package smartparens for dealing with expressions and matched delimiters (e.g. brackets and parantheses) which-key that shows you possible keybindings when you type a partial keybinding. undo-tree: A powerful way to navigate your editing history yasnippet for creating custom snippets and templates much like in vscode. multiple-cursors org-ref for citations Some good config/distributions of Emacs Spacemacs which is a combination of emacs and vim. Purcell Prelude emacs rocks config by Magnar Sveen Remacs which is emacs ported to rust instead of C. Scimax an emacs starter kit for scientists and engineers for reproducible research and publishing, with a focus on LaTeX and org-mode. Graphene for woo users of GUI IDEs like Sublime. Aquamacs and their github page. This is similar to the previous one but better for macOS users. configs of the owner of org-roam Doom private/default binds SPC u as an alternative to C-u. If you want to disable evil hijacking C-u and C-d, do this before evil loads (ie. in private/xarthurx/init.el): To install a package from (M)ELPA, first write (package! example-package) in the packages.el file and just reload doom emacs. Make Emacs better DONE Add tab support to emacs so I can see all open buffers. KILL Read this link doom emacs repo about tramp crashing Resources http://www.jesshamrick.com/2012/09/10/absolute-beginners-guide-to-emacs/ https://www.gnu.org/software/emacs/refcards/pdf/dired-ref.pdf https://blog.insightdatascience.com/emacs-for-data-science-af814b78eb41 Emacs rocks https://tecosaur.github.io/emacs-config/config.html https://tuhdo.github.io/emacs-tutor.html If you want to start from scratch, you can use better deafults. A good article about why you should by into the emacs platform. A glossary for emacs. What features every editor must have and how emacs does it on quora which is also availbe on this link. A good video showing some stuff in emacs by Howard Abrams. A tutorial for emacs calculator. Emacs as a python IDE. A youtube toturial to learn emacs from beginning. Yet another intro to emacs Why somethings should be difficult to use? Emacs Nifty Tricks Emacs Crash Course Tags and getting things done You can use getting things done tags simply by creating a new heading and use of the specified tags such as: TODO do this PROJ eager STRT Tasks that I have started. I\u0026rsquo;m reading a paper WAIT Tasks that require something else from someone else. image segmentation HOLD Tasks that are on hold. email from ED DONE Taks that are finished. Obsidian config KILL Tasks that are canceled. make emacs perfect [?] I don\u0026rsquo;t know TODO you can specify deadlines by typing the following or simply use C-c C-d and then select a date. TODO You can schedule tasks by typing the following or simply use C-c C-s and the select a date. You can change these tags simply by shift+left or right arrow Using C-c C-t you can change the states what tags do I need/have? DONE Create a file for each tag? Or create a file called todo just to put the stuff that I need to do but I don\u0026rsquo;t know where to put? I can use short-term-memory for that, right? yes! Do we have tags or something like that :tags: ? Every headline can contain a list of tags; they occur at the end of the headline. Tags are normal words containing letters, numbers, ‘_’, and ‘@’. Tags must be preceded and followed by a single colon, e.g., ‘:work:’. Several tags can be specified, as in ‘:work:urgent:’. Tags by default are in bold face with the same color as the headline. :tags:\nDo we have #tags as well? Get The Following Done [2/3] [66%] DONE figure out cut DONE why headings don\u0026rsquo;t look different DEADLINE: \u0026lt;2020-12-28 Mon\u0026gt; This is done by a package called org-bullets. However, in doom emacs you can easily use org +pretty in init.el\nTODO Mind map based on the headings same as in Obsidian Commands You can use shift-up for increasing date by 1 day and shift-down for decreasing it if you are inside the date \u0026lt; \u0026gt; otherwise it changes the priority. shift-right and shift-left changes the type of the item: TODO, DONE, WAIT, STRT, PROJ, HOLD, KILL Something TODO Use shift to select text in org mode and another key to change priority and task check boxes [3/3] [100%] cut paste with cmd headings are different tree\nCode blocks in org mode Use the headings shown in example below to write a code inside org file. Alternatively, you could type \u0026lt;s and then hit tab to automatically inser t a code block. To run the code use: C-c C-c For python you have to use RET to see the results, otherwise it always says : None Example return \u0026#39;Hello Emacs and org mode!\u0026#39; LaTeX It\u0026rsquo;s perfect and I can have equation numbering as well. You don\u0026rsquo;t even need to add the $ sign dude. If you hit enter in normal mode when you are inside a latex part, it renders it. How cool is that? Let\u0026rsquo;s try an inline math first: \\(x^2 + y^2 =1\\).\nLet\u0026rsquo;s try it without $ sign:\n\\begin{equaiton}\\label{eq:org} p(a \\vert b) = \\frac{p(a,b)}{p(b)} \\end{equaiton}\nCan I refer to this equation with \\ref{eq:org}?\nNow, let\u0026rsquo;s try an elaborate formula with number.\n\\[\\begin{equation}\\label{my-eq} e = mc^2 \\end{equation}\\]\nin equation \\(\\ref{my-eq}\\) we have einstein.\nFigures and Graphics You have to options to insert and link to graphs in your org files.\nUsing latex\n\\begin{figure}[ht] \\centering \\includegraphics[width=1.0\\columnwidth]{/Volumes/GoogleDrive/My Drive/Apps-Storage/Prime/BrainPrime/Resources/Research/Paper-Summary-Figures/Variational-Sequence-Labelers.png} \\caption{VSLs} \\label{fig:vsls} \\end{figure}\nThen you can link to it by \\(\\ref{fig:vsls}\\).\nUsing this method results in a correct numbering of figures while in the next mehtod it\u0026rsquo;s a bit harder.\nUsing org-ref\nYou can simply link to graph 1 by org\u0026rsquo;s link syntax which is [[figure's label][custom name]].\nYou can easily add the graph by the following syntax.\nCitations and org-ref you can simply use org-ref-helm-insert-cite-link or C-c ] to search for the citation you want to add.\nMarkdown $x_2^2 + x_1^2 =1$ Mermaid and diagrams \u0026lt;/ox-hugo/deleteme.pdf\u0026gt;\nExport There are packages prefixed with ox (org export).\nox-twbs: export a good looking html. ox-gfm: export to github flavored markdowon Use C-c C-e to see the export menu PDF When exporting to pdf thorugh latex using command org-latex-export-to-pdf, it deletes all extra intermediary files (aux, log, etc) by itself.\nHTML Use the command org-html-export-to-html. This is better than latex and is more readable.\nBeamer use a package called ox-beamer to make presentation-like pdfs.\nTables To create a table just type | some | data |RET |- tab data Some 12 hi 34 bye Use shift-right to swap a cell with the cell in its right. Use M-right to move a column to right. TODO I have to find a way to let me have tables not go to next line. Org-Agenda You have to add files to org-agenda-files so that tasks can be shown there. To do so use C-c [. You can also remove files from org-agenda-files by C-c ] TODO how to use org-agenda and calendar for scheduling and time tracking? TODO how to use habit tracker of org mode? DONE in order to add closed date when you mark a task as DONE or KILL, you can add (setq org-log-done 'time) to your config files. Zettelkasten in org mode The following packages are available to help you\norg-zettelkasten which is not maintained anymore org-roam which is my favorite so far. Their website. org-brain: I don\u0026rsquo;t like this one. Very ugly. zetteldeft Org-Roam You can have automatic file renaming using org-roam. For that you only need to change the title of the file you want to rename. Change the title, but also use M-x rename-file and that would update the org-roam database and all the backlinks.\nDONE figure out why org-roam graph doesn\u0026rsquo;t work? It makes the graph as SVG file, so the problem is not with org-roam but rather doom emacs or grpahviz that makes the graph. It can open inkspace svg files.\nYou can use org-roam-server to see the knowledge graph. To do so just run the command org-roam-server-open which you defined that is different from org-roam-server-mode because you have to disable smart-parens since they have conflict. After running this command you can open a tab in your browser with this url http://127.0.0.1:8080/ and see your graph. TODO change the format of captured files to not use dates. Introduction to org-roam by its author References A great intro to org mode by Harry Schwartz. Also check his dot files and emacs config. A list of talks from EmacsNYC. Emacs rocks The ultimate guide to organize your life in plain text with org. Citation in emacs org mode using org-ref. A short read on productivity with org mode. Writing PhD thesis in org mode org tutorials some org mode features you may not know ",
    "ref": "/blog/blogposts/org-mode/"
  },{
    "title": "Podcasts List",
    "date": "",
    "description": "",
    "body": "Podcasts Name Status Genre Author Start Date Rating Episodes channelB #WIP Story, Fiction Ali Bandari 2021-04-13 5/5 Bplus #WIP Book Summary Ali Bandari 5/5 ",
    "ref": "/blog/blogposts/podcasts/"
  }]
